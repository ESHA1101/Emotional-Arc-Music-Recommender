{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc0ace",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EmotionalArcExtractor:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Load CLAP model for embeddings\n",
    "        print(\"⏳ Loading CLAP model (this takes ~30 seconds)...\")\n",
    "        self.clap_model = ClapModel.from_pretrained(\"laion/larger_clap_music_and_speech\")\n",
    "        self.clap_processor = ClapProcessor.from_pretrained(\"laion/larger_clap_music_and_speech\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.clap_model = self.clap_model.to('cuda')\n",
    "            print(\"✅ CLAP loaded on GPU\")\n",
    "        else:\n",
    "            print(\"✅ CLAP loaded on CPU\")\n",
    "\n",
    "    def extract_static_features(self, audio, sr):\n",
    "        \"\"\"Extract CLAP embedding + basic audio features\"\"\"\n",
    "\n",
    "        # 1. CLAP Embedding (512-d)\n",
    "        # Resample to 48kHz for CLAP\n",
    "        if sr != 48000:\n",
    "            audio_48k = librosa.resample(audio, orig_sr=sr, target_sr=48000)\n",
    "        else:\n",
    "            audio_48k = audio\n",
    "\n",
    "        inputs = self.clap_processor(\n",
    "            audios=audio_48k,\n",
    "            sampling_rate=48000,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clap_model.get_audio_features(**inputs)\n",
    "            embedding = embedding.cpu().numpy()[0]  # Shape: (512,)\n",
    "\n",
    "        # 2. Basic audio features\n",
    "        tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sr))\n",
    "        rms = np.mean(librosa.feature.rms(y=audio))\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(audio))\n",
    "\n",
    "        return {\n",
    "            'clap_embedding': embedding,\n",
    "            'tempo': float(tempo),\n",
    "            'spectral_centroid_mean': float(spectral_centroid),\n",
    "            'rms_mean': float(rms),\n",
    "            'zcr_mean': float(zcr)\n",
    "        }\n",
    "\n",
    "    def extract_temporal_features(self, audio, sr, n_windows=10):\n",
    "\n",
    "        # Split audio into windows\n",
    "        window_length = len(audio) // n_windows\n",
    "\n",
    "        valence_trajectory = []\n",
    "        arousal_trajectory = []\n",
    "        spectral_rolloff_trajectory = []\n",
    "\n",
    "        for i in range(n_windows):\n",
    "            start = i * window_length\n",
    "            end = start + window_length\n",
    "            window = audio[start:end]\n",
    "\n",
    "            # Valence proxy: Spectral centroid (brightness)\n",
    "            centroid = np.mean(librosa.feature.spectral_centroid(y=window, sr=sr))\n",
    "            valence_trajectory.append(float(centroid))\n",
    "\n",
    "            # Arousal proxy: RMS energy\n",
    "            energy = np.mean(librosa.feature.rms(y=window))\n",
    "            arousal_trajectory.append(float(energy))\n",
    "\n",
    "            # Spectral rolloff\n",
    "            rolloff = np.mean(librosa.feature.spectral_rolloff(y=window, sr=sr))\n",
    "            spectral_rolloff_trajectory.append(float(rolloff))\n",
    "\n",
    "        # Normalize trajectories to [0, 1]\n",
    "        def normalize(arr):\n",
    "            arr = np.array(arr)\n",
    "            if arr.max() == arr.min():\n",
    "                return [0.5] * len(arr)\n",
    "            return ((arr - arr.min()) / (arr.max() - arr.min())).tolist()\n",
    "\n",
    "        valence_norm = normalize(valence_trajectory)\n",
    "        arousal_norm = normalize(arousal_trajectory)\n",
    "\n",
    "        # Classify the arc type\n",
    "        arc_type, arc_metrics = self.classify_arc(valence_norm, arousal_norm)\n",
    "\n",
    "        return {\n",
    "            'valence_trajectory': valence_norm,\n",
    "            'arousal_trajectory': arousal_norm,\n",
    "            'spectral_rolloff_trajectory': normalize(spectral_rolloff_trajectory),\n",
    "            'arc_type': arc_type,\n",
    "            'arc_slope_valence': arc_metrics['valence_slope'],\n",
    "            'arc_slope_arousal': arc_metrics['arousal_slope'],\n",
    "            'arc_variance_valence': arc_metrics['valence_var'],\n",
    "            'arc_variance_arousal': arc_metrics['arousal_var']\n",
    "        }\n",
    "\n",
    "    def classify_arc(self, valence, arousal):\n",
    "\n",
    "        # Calculate slopes\n",
    "        x = np.arange(len(arousal))\n",
    "        arousal_slope = np.polyfit(x, arousal, 1)[0]\n",
    "        valence_slope = np.polyfit(x, valence, 1)[0]\n",
    "\n",
    "        # Calculate variance\n",
    "        arousal_var = np.var(arousal)\n",
    "        valence_var = np.var(valence)\n",
    "\n",
    "        # Check for spike in latter half\n",
    "        latter_half_arousal = arousal[len(arousal)//2:]\n",
    "        has_spike = max(latter_half_arousal) > (np.mean(arousal) + np.std(arousal))\n",
    "\n",
    "        # Classification logic\n",
    "        if arousal_slope > 0.05 and arousal_var < 0.1:\n",
    "            arc_type = \"Builder\"\n",
    "        elif arousal_slope < -0.05 and arousal_var < 0.1:\n",
    "            arc_type = \"Fader\"\n",
    "        elif valence_var > 0.15:\n",
    "            arc_type = \"Rollercoaster\"\n",
    "        elif arousal_var < 0.05 and valence_var < 0.05:\n",
    "            arc_type = \"Steady\"\n",
    "        elif has_spike:\n",
    "            arc_type = \"Explosive\"\n",
    "        else:\n",
    "            arc_type = \"Dynamic\"\n",
    "\n",
    "        metrics = {\n",
    "            'arousal_slope': float(arousal_slope),\n",
    "            'valence_slope': float(valence_slope),\n",
    "            'arousal_var': float(arousal_var),\n",
    "            'valence_var': float(valence_var)\n",
    "        }\n",
    "\n",
    "        return arc_type, metrics\n",
    "\n",
    "    def process_track(self, audio, sr):\n",
    "\n",
    "        static = self.extract_static_features(audio, sr)\n",
    "        temporal = self.extract_temporal_features(audio, sr)\n",
    "\n",
    "        # Combine into single dict\n",
    "        return {**static, **temporal}\n",
    "\n",
    "# Initialize extractor\n",
    "print(\"Initializing EmotionalArcExtractor...\")\n",
    "extractor = EmotionalArcExtractor()\n",
    "print(\"Extractor ready to process tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3019b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test on track ID 2 \n",
    "print(\"Testing feature extraction on Track ID 2...\")\n",
    "\n",
    "# Load full 30-second clip this time\n",
    "y, sr = librosa.load(loader.get_path(2), sr=22050, duration=30)\n",
    "print(f\"Loaded: {len(y)/sr:.1f} seconds at {sr} Hz\")\n",
    "\n",
    "# Extract features\n",
    "print(\"\\n⏳ Extracting features...\")\n",
    "features = extractor.process_track(y, sr)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTED FEATURES:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n Track: {tracks.loc[2, ('track', 'title')]}\")\n",
    "print(f\"Genre: {tracks.loc[2, ('track', 'genre_top')]}\")\n",
    "\n",
    "print(f\"\\n STATIC FEATURES:\")\n",
    "print(f\"  • CLAP embedding shape: {features['clap_embedding'].shape}\")\n",
    "print(f\"  • Tempo: {features['tempo']:.1f} BPM\")\n",
    "print(f\"  • Spectral Centroid: {features['spectral_centroid_mean']:.1f} Hz\")\n",
    "print(f\"  • RMS Energy: {features['rms_mean']:.4f}\")\n",
    "\n",
    "print(f\"\\n TEMPORAL FEATURES (THE NOVEL PART):\")\n",
    "print(f\"  • Arc Type: {features['arc_type']}\")\n",
    "print(f\"  • Arousal Slope: {features['arc_slope_arousal']:.4f}\")\n",
    "print(f\"  • Valence Slope: {features['arc_slope_valence']:.4f}\")\n",
    "print(f\"  • Arousal Variance: {features['arc_variance_arousal']:.4f}\")\n",
    "print(f\"  • Valence Variance: {features['arc_variance_valence']:.4f}\")\n",
    "\n",
    "print(f\"\\n VALENCE TRAJECTORY (10 windows):\")\n",
    "print(\"  \" + \" → \".join([f\"{v:.2f}\" for v in features['valence_trajectory']]))\n",
    "\n",
    "print(f\"\\n AROUSAL TRAJECTORY (10 windows):\")\n",
    "print(\"  \" + \" → \".join([f\"{v:.2f}\" for v in features['arousal_trajectory']]))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" FEATURE EXTRACTION WORKING PERFECTLY\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
