{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d587783",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class BatchProcessor:\n",
    "    def __init__(self, loader, extractor, tracks_df, project_dir):\n",
    "        self.loader = loader\n",
    "        self.extractor = extractor\n",
    "        self.tracks = tracks_df\n",
    "        self.project_dir = project_dir\n",
    "        self.checkpoint_dir = f\"{project_dir}/checkpoints\"\n",
    "\n",
    "    def get_track_ids(self):\n",
    "        return list(self.tracks.index)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        checkpoint_file = f\"{self.checkpoint_dir}/processed_data.pkl\"\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            with open(checkpoint_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            print(f\"Loaded checkpoint: {len(data)} tracks already processed\")\n",
    "            return data\n",
    "        return []\n",
    "\n",
    "    def save_checkpoint(self, data, batch_num):\n",
    "        checkpoint_file = f\"{self.checkpoint_dir}/processed_data.pkl\"\n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Checkpoint saved: {len(data)} tracks (Batch {batch_num})\")\n",
    "\n",
    "    def process_all(self, batch_size=500):\n",
    "\n",
    "        # Load existing progress\n",
    "        processed_data = self.load_checkpoint()\n",
    "        processed_ids = set([d['track_id'] for d in processed_data])\n",
    "\n",
    "        # Get all track IDs\n",
    "        all_ids = self.get_track_ids()\n",
    "        remaining_ids = [tid for tid in all_ids if tid not in processed_ids]\n",
    "\n",
    "        if len(remaining_ids) == 0:\n",
    "            print(\"All tracks already processed!\")\n",
    "            return processed_data\n",
    "\n",
    "        # Process in batches\n",
    "        batch_num = len(processed_ids) // batch_size\n",
    "\n",
    "        for i in tqdm(range(len(remaining_ids)), desc=\"Processing tracks\"):\n",
    "            track_id = remaining_ids[i]\n",
    "\n",
    "            try:\n",
    "                # Load audio (full 30 seconds)\n",
    "                audio, sr = librosa.load(\n",
    "                    self.loader.get_path(track_id),\n",
    "                    sr=22050,\n",
    "                    duration=30\n",
    "                )\n",
    "\n",
    "                if audio is None or len(audio) < sr * 5:  # Skip if < 5 seconds\n",
    "                    continue\n",
    "\n",
    "                # Extract features\n",
    "                features = self.extractor.process_track(audio, sr)\n",
    "\n",
    "                # Get metadata\n",
    "                try:\n",
    "                    genre = self.tracks.loc[track_id, ('track', 'genre_top')]\n",
    "                    title = self.tracks.loc[track_id, ('track', 'title')]\n",
    "                    artist = self.tracks.loc[track_id, ('artist', 'name')]\n",
    "                except:\n",
    "                    genre = 'Unknown'\n",
    "                    title = 'Unknown'\n",
    "                    artist = 'Unknown'\n",
    "\n",
    "                # Store everything\n",
    "                processed_data.append({\n",
    "                    'track_id': track_id,\n",
    "                    'genre': genre,\n",
    "                    'title': title,\n",
    "                    'artist': artist,\n",
    "                    **features\n",
    "                })\n",
    "\n",
    "                # Save checkpoint every batch_size tracks\n",
    "                if (i + 1) % batch_size == 0:\n",
    "                    batch_num += 1\n",
    "                    self.save_checkpoint(processed_data, batch_num)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n Error processing track {track_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Final save\n",
    "        self.save_checkpoint(processed_data, batch_num + 1)\n",
    "\n",
    "        return processed_data\n",
    "\n",
    "# Initialize processor\n",
    "processor = BatchProcessor(loader, extractor, tracks, project_dir)\n",
    "print(\"Batch processor ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b379ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 1. Re-initialize processor with the CLEAN 8k list\n",
    "processor = BatchProcessor(loader, extractor, tracks_small, project_dir)\n",
    "\n",
    "print(f\"STARTING BATCH PROCESSING ON {len(tracks_small)} TRACKS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Monitor the first 1% to ensure no errors appear.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 2. Execute\n",
    "processed_data = processor.process_all(batch_size=500)\n",
    "\n",
    "# 3. Summary\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"PROCESSING COMPLETE\")\n",
    "print(f\"Time taken: {elapsed:.1f} minutes\")\n",
    "print(f\"Total tracks processed: {len(processed_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa340682",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define path to the file \n",
    "project_dir = ''path''\n",
    "checkpoint_path = f\"{project_dir}/checkpoints/processed_data.pkl\"\n",
    "\n",
    "print(f\"Loading raw data from: {checkpoint_path}\")\n",
    "\n",
    "with open(checkpoint_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Convert list of dictionaries to a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the \"complete\" version so it's easier next time\n",
    "df.to_pickle(f'{project_dir}/fma_features_complete.pkl')\n",
    "\n",
    "print(f\" SUCCESS! Loaded {len(df)} tracks.\")\n",
    "print(f\"   Variable 'df' is ready to use.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
